include:
  - docker-compose.yml

services:

  # vllm service(s)
  vllm-0:
    image: vllm/vllm-openai:latest
    entrypoint: [
      "vllm", "serve", "NousResearch/Hermes-3-Llama-3.1-8B",
      "--enable-prefix-caching", "--enable-chunked-prefill",
      "--gpu-memory-utilization", "0.98",
      "--max-model-len", "131072",
      "--enable-auto-tool-choice", "--tool-call-parser", "hermes"
    ]
    environment:
      HUGGING_FACE_HUB_TOKEN: ${HUGGINGFACE_TOKEN:?error}
    volumes:
      - type: bind
        source: ${HF_HOME:?error}
        target: /root/.cache/huggingface
      - type: bind
        source: ./node/inference/configs
        target: /usr/app # configs like chat templates, vllm configs, tool parsers
    ipc: host
    deploy:
      resources:
        reservations:
          devices:
            - driver: "nvidia"
              capabilities: [ "gpu" ]
              device_ids: [ "0" ]

  vllm-1:
    image: vllm/vllm-openai:latest
    entrypoint: [
      "vllm", "serve", "Qwen/Qwen2.5-7B-Instruct",
      "--enable-prefix-caching", "--enable-chunked-prefill",
      "--gpu-memory-utilization", "0.98",
      "--max-model-len", "32768",
      "--enable-auto-tool-choice", "--tool-call-parser", "hermes"
    ]
    environment:
      - HUGGING_FACE_HUB_TOKEN=${HUGGINGFACE_TOKEN}
    volumes:
      - type: bind
        source: ${HF_HOME:?error}
        target: /root/.cache/huggingface
      - type: bind
        source: ./node/inference/configs
        target: /usr/app # configs like chat templates, vllm configs, tool parsers
    ipc: host
    restart: unless-stopped
    deploy:
      resources:
        reservations:
          devices:
            - driver: "nvidia"
              capabilities: [ "gpu" ]
              device_ids: [ "1" ]

  vllm-2:
    image: vllm/vllm-openai:latest
    entrypoint: [
      "vllm", "serve", "meta-llama/Llama-3.1-8B-Instruct",
      "--enable-prefix-caching", "--enable-chunked-prefill",
      "--gpu-memory-utilization", "0.98",
      "--max-model-len", "131072",
      "--enable-auto-tool-choice", "--tool-call-parser", "llama3_json",
      "--chat-template", "/usr/app/chat-templates/llama_3_1.jinja"
    ]
    environment:
      - HUGGING_FACE_HUB_TOKEN=${HUGGINGFACE_TOKEN}
    volumes:
      - type: bind
        source: ${HF_HOME:?error}
        target: /root/.cache/huggingface
      - type: bind
        source: ./node/inference/configs
        target: /usr/app # configs like chat templates, vllm configs, tool parsers
    ipc: host
    restart: unless-stopped
    deploy:
      resources:
        reservations:
          devices:
            - driver: "nvidia"
              capabilities: [ "gpu" ]
              device_ids: [ "2" ]

  vllm-3:
    image: vllm/vllm-openai:latest
    entrypoint: [
      "vllm", "serve", "Team-ACE/ToolACE-8B",
      "--enable-prefix-caching", "--enable-chunked-prefill",
      "--gpu-memory-utilization", "0.98",
      "--max-model-len", "131072",
      "--enable-auto-tool-choice", "--tool-call-parser", "pythonic",
      "--tool-parser-plugin", "/usr/app/tool-parsers/pythonic_tool_parser.py",
      "--chat-template", "/usr/app/chat-templates/tool_ace.jinja"
    ]
    environment:
      - HUGGING_FACE_HUB_TOKEN=${HUGGINGFACE_TOKEN}
    volumes:
      - type: bind
        source: ${HF_HOME:?error}
        target: /root/.cache/huggingface
      - type: bind
        source: ./node/inference/configs
        target: /usr/app # configs like chat templates, vllm configs, tool parsers
    ipc: host
    restart: unless-stopped
    deploy:
      resources:
        reservations:
          devices:
            - driver: "nvidia"
              capabilities: [ "gpu" ]
              device_ids: [ "3" ]

  vllm-4:
    image: vllm/vllm-openai:latest
    entrypoint: [
      "vllm", "serve", "ibm-granite/granite-3.1-8b-instruct",
      "--enable-auto-tool-choice", "--enable-chunked-prefill", "--enable-prefix-caching",
      "--gpu-memory-utilization", "0.98",
      "--max-model-len", "32768",
      "--tool-call-parser", "granite",
    ]
    environment:
      - HUGGINGFACE_HUB_TOKEN=${HUGGINGFACE_TOKEN}
    volumes:
      - type: bind
        source: ${HF_HOME:?error}
        target: /root/.cache/huggingface
      - type: bind
        source: ./node/inference/configs
        target: /usr/app # configs like chat templates, vllm configs, tool parsers
    ipc: host
    restart: unless-stopped
    deploy:
      resources:
        reservations:
          devices:
            - driver: "nvidia"
              capabilities: ["gpu"]
              device_ids: ["4"]

  vllm-5:
    image: vllm/vllm-openai:latest
    entrypoint: [
      "vllm", "serve", "internlm/internlm2_5-7b-chat",
      "--enable-prefix-caching", "--enable-auto-tool-choice", "--enable-chunked-prefill",
      "--gpu-memory-utilization", "0.98",
      "--max-model-len", "65536",
      "--tool-call-parser", "internlm",
      "--chat-template", "/usr/app/chat-templates/internlm.jinja",
      "--trust-remote-code"
    ]
    environment:
      - HUGGING_FACE_HUB_TOKEN=${HUGGINGFACE_TOKEN}
    volumes:
      - type: bind
        source: ${HF_HOME:?error}
        target: /root/.cache/huggingface
      - type: bind
        source: ./node/inference/configs
        target: /usr/app # configs like chat templates, vllm configs, tool parsers
    ipc: host
    restart: unless-stopped
    deploy:
      resources:
        reservations:
          devices:
            - driver: "nvidia"
              capabilities: [ "gpu" ]
              device_ids: [ "5" ]

  vllm-6:
    image: vllm/vllm-openai:latest
    entrypoint: [
      "vllm", "serve", "meetkai/functionary-small-v3.1",
      "--enable-auto-tool-choice", "--enable-chunked-prefill", # no prefix caching bc sliding window
      "--tool-parser-plugin", "/usr/app/tool-parsers/llama3_xml.py", # uses llama 3.1's XML-like format
      "--tool-call-parser", "functionary_31",
      "--gpu-memory-utilization", "0.98",
      "--max-model-len", "131072",
    ]
    environment:
      - HUGGING_FACE_HUB_TOKEN=${HUGGINGFACE_TOKEN}
    volumes:
      - type: bind
        source: ${HF_HOME:?error}
        target: /root/.cache/huggingface
      - type: bind
        source: ./node/inference/configs
        target: /usr/app # configs like chat templates, vllm configs, tool parsers
    ipc: host
    restart: unless-stopped
    deploy:
      resources:
        reservations:
          devices:
            - driver: "nvidia"
              capabilities: [ "gpu" ]
              device_ids: [ "6" ]

  vllm-7:
    image: vllm/vllm-openai:latest
    entrypoint: [
      "vllm", "serve", "BAAI/bge-base-en-v1.5"
    ]
    environment:
      - HUGGING_FACE_HUB_TOKEN=${HUGGINGFACE_TOKEN}
    volumes:
      - type: bind
        source: ${HF_HOME:?error}
        target: /root/.cache/huggingface
      - type: bind
        source: ./node/inference/configs
        target: /usr/app # configs like chat templates, vllm configs, tool parsers
    ipc: host
    restart: unless-stopped
    deploy:
      resources:
        reservations:
          devices:
            - driver: "nvidia"
              capabilities: [ "gpu" ]
              device_ids: [ "7" ]


  # litellm
  litellm:
    container_name: litellm
    image: ghcr.io/berriai/litellm:main-latest
    command: --config /app/config.yaml
    restart: unless-stopped
    depends_on:
      pgvector:
        condition: service_healthy
    environment:
      LITELLM_MASTER_KEY: ${LITELLM_MASTER_KEY:?error}
      LITELLM_SALT_KEY: ${LITELLM_SALT_KEY:?error}
      DATABASE_URL: postgresql://${LOCAL_DB_POSTGRES_USERNAME?:error}:${LOCAL_DB_POSTGRES_PASSWORD:?error}@pgvector:5432/litellm
    volumes:
      - ./litellm_config.vllm.yml:/app/config.yaml # this should be the litellm config file
    ports:
      - '4000:4000'

networks:
  default:
    name: node-network